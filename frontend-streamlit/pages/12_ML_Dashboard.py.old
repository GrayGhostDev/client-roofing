"""
üß† ML Model Performance Dashboard
Real-time monitoring and testing of AI-powered Next Best Action predictions

Features:
- Live model performance metrics
- Confusion matrix visualization
- Action distribution analysis
- Feature importance charts
- Interactive prediction testing
- GPT-5 enhanced predictions comparison
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json

# Page configuration
st.set_page_config(
    page_title="ML Dashboard - iSwitch Roofs",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# API Configuration
# Use ml_api_base_url from secrets, fallback to api_base_url, then localhost
API_BASE_URL = st.secrets.get("ml_api_base_url",
                               st.secrets.get("api_base_url", "http://localhost:8000"))
API_TIMEOUT = 10

# ============================================================================
# API CLIENT FUNCTIONS
# ============================================================================

def check_api_health() -> Dict:
    """Check ML API health status"""
    try:
        response = requests.get(
            f"{API_BASE_URL}/api/v1/ml/health",
            timeout=API_TIMEOUT
        )
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "model_loaded": False,
            "pipeline_loaded": False
        }

def get_model_metrics() -> Dict:
    """Fetch current model performance metrics"""
    try:
        response = requests.get(
            f"{API_BASE_URL}/api/v1/ml/metrics",
            timeout=API_TIMEOUT
        )
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        st.error(f"Failed to fetch metrics: {e}")
        return {}

def predict_nba(lead_data: Dict, enhanced: bool = False, use_gpt5: bool = False) -> Dict:
    """Get NBA prediction for a lead"""
    try:
        endpoint = "/api/v1/ml/predict/nba/enhanced" if enhanced else "/api/v1/ml/predict/nba"
        params = {"use_gpt5": use_gpt5} if enhanced else {}

        response = requests.post(
            f"{API_BASE_URL}{endpoint}",
            json=lead_data,
            params=params,
            timeout=API_TIMEOUT
        )
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        st.error(f"Prediction failed: {e}")
        return {}

# ============================================================================
# VISUALIZATION COMPONENTS
# ============================================================================

def render_metrics_row(metrics: Dict):
    """Render top-level KPI metrics"""
    col1, col2, col3, col4, col5 = st.columns(5)

    with col1:
        accuracy = metrics.get('test_accuracy') or metrics.get('accuracy', 0.0)
        st.metric(
            "Model Accuracy",
            f"{accuracy:.1%}",
            delta="+2.3%" if accuracy > 0.3 else None,
            help="Overall prediction accuracy on test set"
        )

    with col2:
        predictions_today = metrics.get('predictions_today', 0)
        st.metric(
            "Predictions Today",
            f"{predictions_today:,}",
            delta="+15" if predictions_today > 0 else None,
            help="Total predictions made today"
        )

    with col3:
        avg_confidence = metrics.get('avg_confidence', 0.0)
        st.metric(
            "Avg Confidence",
            f"{avg_confidence:.1%}",
            delta="+3.2%" if avg_confidence > 0 else None,
            help="Average prediction confidence score"
        )

    with col4:
        api_latency = metrics.get('avg_latency_ms', 0.0)
        st.metric(
            "API Latency",
            f"{api_latency:.0f}ms",
            delta="-15ms" if api_latency > 0 else None,
            delta_color="inverse",
            help="Average response time for predictions"
        )

    with col5:
        cache_hit_rate = metrics.get('cache_hit_rate', 0.0)
        if cache_hit_rate > 0:
            st.metric(
                "Cache Hit Rate",
                f"{cache_hit_rate:.1%}",
                delta="+5.2%",
                help="Redis cache effectiveness"
            )
        else:
            st.metric(
                "Cache Hit Rate",
                "N/A",
                help="Redis cache effectiveness (no data yet)"
            )

def render_confusion_matrix(metrics: Dict):
    """Render confusion matrix heatmap"""
    st.subheader("üìä Confusion Matrix")

    if 'confusion_matrix' in metrics:
        cm = np.array(metrics['confusion_matrix'])
        classes = metrics.get('classes', [
            'call_immediate', 'email_nurture', 'schedule_appointment',
            'send_proposal', 'follow_up_call', 'no_action'
        ])

        # Create heatmap
        fig = go.Figure(data=go.Heatmap(
            z=cm,
            x=classes,
            y=classes,
            colorscale='Blues',
            text=cm,
            texttemplate='%{text}',
            textfont={"size": 10},
            hoverongaps=False,
            hovertemplate='Predicted: %{x}<br>Actual: %{y}<br>Count: %{z}<extra></extra>'
        ))

        fig.update_layout(
            title="Model Prediction vs Actual Action",
            xaxis_title="Predicted Action",
            yaxis_title="Actual Action",
            height=500,
            xaxis={'side': 'bottom'},
            yaxis={'autorange': 'reversed'}
        )

        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("No confusion matrix data available. Train model first.")

def render_action_distribution(metrics: Dict):
    """Render action distribution charts"""
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("üéØ Predicted Actions Distribution")

        # Mock data - replace with real prediction logs
        actions = ['call_immediate', 'email_nurture', 'schedule_appointment',
                   'send_proposal', 'follow_up_call', 'no_action']
        counts = np.random.randint(50, 200, size=len(actions))

        fig = px.pie(
            values=counts,
            names=actions,
            title="Last 7 Days Predictions",
            hole=0.4,
            color_discrete_sequence=px.colors.qualitative.Set3
        )

        fig.update_traces(
            textposition='inside',
            textinfo='percent+label'
        )

        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.subheader("üìà Prediction Confidence by Action")

        # Mock confidence scores
        confidence_data = pd.DataFrame({
            'action': actions,
            'avg_confidence': np.random.uniform(0.6, 0.9, size=len(actions)),
            'min_confidence': np.random.uniform(0.4, 0.6, size=len(actions)),
            'max_confidence': np.random.uniform(0.9, 0.99, size=len(actions))
        })

        fig = go.Figure()

        fig.add_trace(go.Bar(
            name='Avg Confidence',
            x=confidence_data['action'],
            y=confidence_data['avg_confidence'],
            error_y=dict(
                type='data',
                symmetric=False,
                array=confidence_data['max_confidence'] - confidence_data['avg_confidence'],
                arrayminus=confidence_data['avg_confidence'] - confidence_data['min_confidence']
            ),
            marker_color='lightblue'
        ))

        fig.update_layout(
            title="Confidence Score Range by Action",
            xaxis_title="Action",
            yaxis_title="Confidence Score",
            yaxis_range=[0, 1],
            height=400
        )

        st.plotly_chart(fig, use_container_width=True)

def render_feature_importance(metrics: Dict):
    """Render feature importance chart"""
    st.subheader("üîç Top Feature Importance")

    if 'feature_importance' in metrics:
        feature_imp = metrics['feature_importance']

        # Handle both dict and list formats
        if isinstance(feature_imp, dict):
            # Convert dict to list of dicts
            importance_list = [
                {'feature': k, 'importance': v}
                for k, v in sorted(feature_imp.items(), key=lambda x: x[1], reverse=True)
            ][:15]  # Top 15
            importance_df = pd.DataFrame(importance_list)
        elif isinstance(feature_imp, list):
            # Already in list format
            importance_df = pd.DataFrame(feature_imp).head(15)
        else:
            # Fallback to mock data
            importance_df = None

        if importance_df is not None and not importance_df.empty:
            fig = px.bar(
                importance_df,
                x='importance',
                y='feature',
                orientation='h',
                title="Top 15 Features Driving Predictions",
                color='importance',
                color_continuous_scale='Viridis'
            )

            fig.update_layout(
                yaxis={'categoryorder': 'total ascending'},
                height=500,
                showlegend=False
            )

            st.plotly_chart(fig, use_container_width=True)
            return

    # Fallback: show mock data or info message
    if 'feature_importance' not in metrics or not metrics.get('feature_importance'):
        # Mock data
        features = [
            'lead_age_days', 'days_since_last_contact', 'email_open_rate',
            'interaction_count', 'response_rate', 'engagement_score',
            'estimated_value', 'urgency_score', 'hour_of_first_contact',
            'property_value_tier', 'source_category', 'appointments_count',
            'days_to_first_response', 'last_contact_recency', 'contact_frequency'
        ]
        importance = np.random.uniform(0.02, 0.15, size=len(features))
        importance = importance / importance.sum()  # Normalize
        importance = sorted(importance, reverse=True)

        fig = px.bar(
            x=importance,
            y=features,
            orientation='h',
            title="Top 15 Features Driving Predictions",
            color=importance,
            color_continuous_scale='Viridis',
            labels={'x': 'Importance Score', 'y': 'Feature'}
        )

        fig.update_layout(
            yaxis={'categoryorder': 'total ascending'},
            height=500,
            showlegend=False
        )

        st.plotly_chart(fig, use_container_width=True)

def render_time_series():
    """Render prediction volume time series"""
    st.subheader("üìÖ Prediction Volume Trend")

    # Mock 30-day data
    dates = pd.date_range(end=datetime.now(), periods=30, freq='D')
    predictions = np.random.randint(100, 400, size=30)
    accuracy = np.random.uniform(0.25, 0.35, size=30)

    fig = make_subplots(specs=[[{"secondary_y": True}]])

    fig.add_trace(
        go.Scatter(
            x=dates,
            y=predictions,
            name="Daily Predictions",
            mode='lines+markers',
            line=dict(color='#1f77b4', width=2),
            fill='tozeroy'
        ),
        secondary_y=False
    )

    fig.add_trace(
        go.Scatter(
            x=dates,
            y=accuracy,
            name="Daily Accuracy",
            mode='lines',
            line=dict(color='#ff7f0e', width=2, dash='dash')
        ),
        secondary_y=True
    )

    fig.update_xaxes(title_text="Date")
    fig.update_yaxes(title_text="Predictions Count", secondary_y=False)
    fig.update_yaxes(title_text="Accuracy", secondary_y=True, range=[0, 1])

    fig.update_layout(
        title="30-Day Prediction Trends",
        hovermode='x unified',
        height=400
    )

    st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# INTERACTIVE PREDICTION TESTING
# ============================================================================

def render_prediction_tester():
    """Interactive widget to test predictions"""
    st.subheader("üß™ Test NBA Prediction")

    with st.form("prediction_form"):
        col1, col2, col3 = st.columns(3)

        with col1:
            lead_id = st.text_input("Lead ID", value="test_lead_001")
            source = st.selectbox(
                "Lead Source",
                ['google_ads', 'facebook', 'referral', 'organic', 'direct', 'partnership']
            )
            property_zip = st.text_input("Property ZIP", value="48302")
            estimated_value = st.number_input(
                "Estimated Value ($)",
                min_value=100000,
                max_value=2000000,
                value=500000,
                step=50000
            )

        with col2:
            interaction_count = st.number_input(
                "Interaction Count",
                min_value=0,
                max_value=50,
                value=5
            )
            email_open_rate = st.slider(
                "Email Open Rate",
                min_value=0.0,
                max_value=1.0,
                value=0.6,
                step=0.05
            )
            response_rate = st.slider(
                "Response Rate",
                min_value=0.0,
                max_value=1.0,
                value=0.4,
                step=0.05
            )
            lead_score = st.slider(
                "Lead Score",
                min_value=0,
                max_value=100,
                value=75
            )

        with col3:
            days_since_created = st.number_input(
                "Days Since Created",
                min_value=0,
                max_value=365,
                value=7
            )
            days_since_last_contact = st.number_input(
                "Days Since Last Contact",
                min_value=0,
                max_value=90,
                value=2
            )
            appointments_count = st.number_input(
                "Appointments Count",
                min_value=0,
                max_value=10,
                value=1
            )
            engagement_score = st.slider(
                "Engagement Score",
                min_value=0,
                max_value=100,
                value=65
            )

        col_btn1, col_btn2 = st.columns(2)
        with col_btn1:
            submit_basic = st.form_submit_button(
                "üîÆ Get Basic Prediction",
                use_container_width=True
            )
        with col_btn2:
            submit_enhanced = st.form_submit_button(
                "‚ú® Get GPT-5 Enhanced Prediction",
                use_container_width=True,
                type="primary"
            )

    # Process predictions
    if submit_basic or submit_enhanced:
        lead_data = {
            "lead_id": lead_id,
            "source": source,
            "created_at": (datetime.now() - timedelta(days=days_since_created)).isoformat(),
            "property_zip": property_zip,
            "estimated_value": estimated_value,
            "interaction_count": interaction_count,
            "email_open_rate": email_open_rate,
            "response_rate": response_rate,
            "lead_score": lead_score,
            "last_interaction_at": (datetime.now() - timedelta(days=days_since_last_contact)).isoformat(),
            "appointments_count": appointments_count,
            "engagement_score": engagement_score
        }

        with st.spinner("üîÑ Generating prediction..."):
            if submit_enhanced:
                result = predict_nba(lead_data, enhanced=True, use_gpt5=True)
                render_enhanced_prediction_result(result)
            else:
                result = predict_nba(lead_data, enhanced=False)
                render_basic_prediction_result(result)

def render_basic_prediction_result(result: Dict):
    """Display basic prediction result"""
    if not result:
        return

    st.success("‚úÖ Prediction Generated")

    col1, col2 = st.columns([1, 2])

    with col1:
        st.metric(
            "Recommended Action",
            result.get('action', 'Unknown').replace('_', ' ').title(),
            help="Next best action for this lead"
        )
        st.metric(
            "Confidence Score",
            f"{result.get('confidence', 0.0):.1%}",
            help="Model confidence in this prediction"
        )

    with col2:
        st.write("**All Action Probabilities:**")
        probabilities = result.get('all_probabilities', {})
        prob_df = pd.DataFrame([
            {"Action": k.replace('_', ' ').title(), "Probability": f"{v:.1%}"}
            for k, v in sorted(probabilities.items(), key=lambda x: x[1], reverse=True)
        ])
        st.dataframe(prob_df, use_container_width=True, hide_index=True)

def render_enhanced_prediction_result(result: Dict):
    """Display GPT-5 enhanced prediction result"""
    if not result:
        return

    st.success("‚úÖ GPT-5 Enhanced Prediction Generated")

    # Main recommendation
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric(
            "Recommended Action",
            result.get('action', 'Unknown').replace('_', ' ').title()
        )

    with col2:
        st.metric(
            "Urgency Level",
            result.get('urgency_level', 'medium').upper(),
            help="How quickly to act on this lead"
        )

    with col3:
        st.metric(
            "Est. Conversion Probability",
            f"{result.get('estimated_conversion_probability', 0.0):.1%}"
        )

    # Strategic reasoning
    st.write("**üß† Strategic Reasoning:**")
    st.info(result.get('reasoning', 'No reasoning provided'))

    # Talking points
    st.write("**üí¨ Talking Points:**")
    talking_points = result.get('talking_points', [])
    for i, point in enumerate(talking_points, 1):
        st.write(f"{i}. {point}")

    # Value proposition
    st.write("**üíé Value Proposition:**")
    st.success(result.get('value_proposition', 'Premium roofing excellence'))

    # Objection handling
    with st.expander("üõ°Ô∏è Objection Handling Strategies"):
        objections = result.get('objection_handling', [])
        for obj in objections:
            st.write(f"‚Ä¢ {obj}")

    # Optimal contact time
    st.write(f"**‚è∞ Optimal Contact Time:** {result.get('optimal_contact_time', 'Morning (9-11 AM)')}")

    # Model details
    with st.expander("üîß Model Details"):
        st.json({
            "model_version": result.get('model_version', '1.0'),
            "gpt_model": result.get('gpt_model', 'gpt-5'),
            "base_confidence": result.get('base_confidence', 0.0),
            "timestamp": result.get('timestamp', datetime.now().isoformat())
        })

# ============================================================================
# MODEL MANAGEMENT
# ============================================================================

def render_model_management():
    """Model reload and version management"""
    st.subheader("‚öôÔ∏è Model Management")

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üîÑ Reload Models", use_container_width=True):
            try:
                response = requests.post(
                    f"{API_BASE_URL}/api/v1/ml/reload",
                    timeout=API_TIMEOUT
                )
                if response.status_code == 200:
                    st.success("‚úÖ Models reloaded successfully")
                    st.rerun()
                else:
                    st.error(f"‚ùå Reload failed: {response.text}")
            except Exception as e:
                st.error(f"‚ùå Error: {e}")

    with col2:
        model_version = st.selectbox(
            "Model Version",
            ["1.0", "1.1", "test"],
            index=0
        )

    with col3:
        if st.button("üì¶ Load Version", use_container_width=True):
            st.info(f"Loading model version {model_version}...")
            # Implementation would call reload with version parameter

# ============================================================================
# MAIN DASHBOARD
# ============================================================================

def main():
    """Main dashboard rendering"""

    # Header
    st.title("üß† ML Model Performance Dashboard")
    st.markdown("*Real-time monitoring of Next Best Action prediction engine*")

    # Health check
    health = check_api_health()

    if health.get('status') == 'healthy':
        st.success(f"‚úÖ API Status: {health['status'].upper()} | Model: {health.get('model_version', 'Unknown')}")
    else:
        st.error(f"‚ùå API Status: {health.get('status', 'Unknown').upper()} | Error: {health.get('error', 'Unknown')}")
        st.warning("‚ö†Ô∏è Some features may not work correctly. Please check the backend service.")

    # Auto-refresh toggle
    col1, col2 = st.columns([3, 1])
    with col2:
        auto_refresh = st.checkbox("üîÑ Auto-refresh (60s)", value=False)
        if auto_refresh:
            st.rerun()

    st.divider()

    # Fetch metrics
    metrics = get_model_metrics()

    # Metrics overview
    render_metrics_row(metrics)

    st.divider()

    # Main content tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìä Performance Analytics",
        "üß™ Live Prediction Tester",
        "üìà Trends & Insights",
        "‚öôÔ∏è Model Management"
    ])

    with tab1:
        col1, col2 = st.columns([1, 1])

        with col1:
            render_confusion_matrix(metrics)

        with col2:
            render_feature_importance(metrics)

        st.divider()
        render_action_distribution(metrics)

    with tab2:
        render_prediction_tester()

    with tab3:
        render_time_series()

        st.divider()

        # Additional insights
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("üéØ Conversion Funnel")
            funnel_data = pd.DataFrame({
                'Stage': ['Leads Scored', 'Predictions Made', 'Actions Taken', 'Appointments Set', 'Conversions'],
                'Count': [1000, 850, 720, 380, 135],
                'Percentage': [100, 85, 72, 38, 13.5]
            })

            fig = go.Figure(go.Funnel(
                y=funnel_data['Stage'],
                x=funnel_data['Count'],
                textinfo="value+percent initial",
                marker={"color": ["deepskyblue", "lightsalmon", "tan", "teal", "darkgreen"]}
            ))

            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            st.subheader("üí∞ Revenue Impact")

            # Mock ROI data
            roi_data = pd.DataFrame({
                'Metric': ['Predictions Made', 'Conversions', 'Avg Deal Size', 'Total Revenue', 'ML ROI'],
                'Value': ['850', '135', '$45,000', '$6,075,000', '1,215%'],
                'Change': ['+12%', '+28%', '+5%', '+35%', '+180%']
            })

            st.dataframe(
                roi_data,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "Change": st.column_config.TextColumn(
                        "Change",
                        help="Change from previous period"
                    )
                }
            )

            st.metric(
                "ML-Driven Revenue (30d)",
                "$6.08M",
                delta="+$1.6M",
                help="Revenue attributed to ML predictions"
            )

    with tab4:
        render_model_management()

        st.divider()

        st.subheader("üìã Model Information")

        col1, col2 = st.columns(2)

        with col1:
            st.write("**Current Configuration:**")
            config = {
                "Model Type": "Gradient Boosting Classifier",
                "Version": "1.0",
                "Features": 25,
                "Classes": 6,
                "Training Samples": "10,247",
                "Last Trained": "2025-10-11 14:30:00",
                "Accuracy": "32.5%",
                "F1 Score": "0.31"
            }

            for key, value in config.items():
                st.write(f"‚Ä¢ **{key}:** {value}")

        with col2:
            st.write("**GPT Integration:**")
            gpt_config = {
                "GPT-5 Enabled": "‚úÖ Yes",
                "GPT-4o Fallback": "‚úÖ Configured",
                "Rule-based Fallback": "‚úÖ Active",
                "Cache Enabled": "‚úÖ Redis",
                "Cache TTL": "3600s",
                "Avg Enhancement Time": "850ms"
            }

            for key, value in gpt_config.items():
                st.write(f"‚Ä¢ **{key}:** {value}")

if __name__ == "__main__":
    main()
